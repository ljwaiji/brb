# BRB - Belief Rule Base System with PyTorch Implementation

A PyTorch implementation of Belief Rule Base (BRB) system for inference and prediction tasks. This project focuses on implementing the Extended Belief Rule Base (EBRB) methodology for regression problems, with an application to pipeline leak detection.

## Overview

Belief Rule Base (BRB) is a rule-based inference methodology that combines fuzzy logic and the Dempster-Shafer theory of evidence. It is particularly useful for handling complex nonlinear relationships and uncertainty in data.

This implementation includes:
- A PyTorch-based BRB model for regression tasks
- The Adan optimizer for efficient training
- Utilities for creating and configuring BRB systems
- A sample application for pipeline leak detection

## Components

- `ebrb_torch.py`: Core implementation of the Extended Belief Rule Base system in PyTorch, including:
  - `Rule_Base`: Class for defining the BRB structure
  - `Inference`: Class implementing the evidential reasoning algorithm
  - `triangle_antc_match_GPU`: Function for matching input data to antecedent references
- `train.py`: Script for training the BRB model on pipeline leak detection data, with:
  - Data preprocessing and normalization
  - Model initialization
  - Training loop with batch processing
  - Validation on test dataset
- `utils.py`: Utility functions for creating BRB models, including:
  - `create_brb`: Function to initialize a BRB model with appropriate parameters
- `adan.py`: Implementation of the Adan optimizer (Adaptive Nesterov Momentum Algorithm)
- `oil.data`: Sample dataset for pipeline leak detection with three columns (pressure, flow rate, leak status)

## BRB Methodology

The Belief Rule Base (BRB) methodology consists of:

1. **Rule Structure**: Each rule in a BRB has the form:
   ```
   IF antecedent THEN consequent with belief degrees
   ```

2. **Matching Degree Calculation**: Input data is matched to antecedent attributes using triangular membership functions.

3. **Activation Weight Calculation**: Rules are activated based on matching degrees and attribute weights.

4. **Belief Inference Using ER Algorithm**: The Evidential Reasoning (ER) algorithm is used to combine the activated rules.

5. **Output Generation**: Final output is generated by combining belief degrees with consequent references.

## Getting Started

### Prerequisites
- Python 3.6+
- PyTorch 1.8+
- NumPy
- scikit-learn

### Installation

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/brb.git
   cd brb
   ```

2. Install required packages:
   ```
   pip install torch numpy scikit-learn
   ```

### Usage

1. Prepare your dataset in a similar format to `oil.data` (features and target values).

2. Customize the training parameters in `train.py`:
   ```python
   # Key parameters to consider
   reference_num = 7  # Number of reference values for antecedent attributes
   L = 56             # Number of rules
   N = 5              # Number of consequent reference values
   T = 2              # Number of antecedent attributes
   epochs = 1000      # Training epochs
   batch_size = 32    # Batch size
   ```

3. Run the training script:
   ```
   python train.py
   ```

4. Monitor the training process:
   ```
   Epoch X/1000 | Train Loss: X.XXXX | Test Loss: X.XXXX
   ```

## Data Preprocessing

The training script includes the following preprocessing steps:

1. **Loading data**: Data is loaded from the `oil.data` file.
2. **Feature normalization**: Input features are standardized (zero mean, unit variance).
3. **Target normalization**: Target values are standardized.
4. **Train-test split**: Data is split into training (80%) and test (20%) sets.

## Model Configuration

The BRB model in this implementation uses:

- **Reference values**: Generated from data range for each antecedent attribute
- **Rule weights (theta)**: Initialized to ones, trainable parameters
- **Attribute weights (delta)**: Initialized to ones, trainable parameters
- **Belief degrees (beta)**: Initialized randomly, trainable parameters

## Training Process

The training process includes:

1. **Batch processing**: Data is processed in mini-batches.
2. **Forward pass**: Input data is matched to reference values, and BRB inference is performed.
3. **Loss calculation**: Mean squared error between predicted and actual values.
4. **Parameter update**: Adan optimizer updates model parameters.
5. **Constraint handling**: Parameters are constrained to valid ranges (e.g., [0,1] for weights).
6. **Validation**: Model is tested on the test set after each epoch.

## Customization

To adapt this implementation to other datasets or problems:

1. Modify the data loading section in `train.py` to use your own dataset.
2. Adjust the number of antecedent attributes (`T`) to match your input features.
3. Tune hyperparameters such as `reference_num`, `L`, and `N` based on your problem complexity.
4. For classification tasks, change the loss function and set `task='classification'` in the Inference class.

## Reference

This implementation is based on research in belief rule-based expert systems, particularly for pipeline leak detection as described in:
- "Inference and learning methodology of belief-rule-based expert system for pipeline leak detection" (2006)
- Other related literature on evidential reasoning and belief rule base systems

## License